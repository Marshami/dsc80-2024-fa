{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07928a6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201f07b",
   "metadata": {},
   "source": [
    "## Midterm Redemption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc8ba3",
   "metadata": {},
   "source": [
    "### Question 0\n",
    "\n",
    "The exam redemption consists of 8 questions worth a total of 25 points. Please complete the exam redemption. Here are a few things that differ from the midterm:\n",
    "\n",
    "Instead of assigning variables directly using the correct code, we convert them into functions. Each function takes no arguments but should return the code you would normally assign to the variable. Hereâ€™s an example:\n",
    "In the exam, you wrote:\n",
    "```python\n",
    "answer = 1 + 1\n",
    "```\n",
    "\n",
    "But in the redemption, you will write:\n",
    "\n",
    "```python\n",
    "def answer_function():\n",
    "    return 1 + 1\n",
    "```\n",
    "\n",
    "Please refer to the lab.py file to see the names of the functions corresponding to each question.\n",
    "\n",
    "Feel free to post any logistical questions, but course staff will not answer questions related to the answers.\n",
    "\n",
    "Start by running the cell below to import the packages needed for the exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "58f1cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "76e15167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "298d68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from dsc80_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c662454",
   "metadata": {},
   "source": [
    "## Parks!\n",
    "Sam is working with a dataset on visits to US National Parks. There are two CSV files that he's working with: `data/parks.csv` and `data/visits.csv`. The columns of each dataset are described below.\n",
    "\n",
    "`data/parks.csv`:\n",
    "\n",
    "- `park_id` (`int`): A unique identifier for a park.\n",
    "- `parkname` (`str`): The name of each park. `NP` is short for National Park. `PRES` is short for Preserve.\n",
    "- `region` (`str`): The region of the US that the park is in. \n",
    "- `state` (`str`): The two-letter abbreviation for the state that the park is in.\n",
    "\n",
    "`data/visits.csv`:\n",
    "\n",
    "- `name` (`str`): The full name of the park visitor. You can assume that no two people share the same first and list name.\n",
    "- `entry` (`datetime64`): The timestamp that the visitor entered the park.\n",
    "- `duration` (`int`): The number of hours that each person stayed at the park.\n",
    "- `park_id` (`int`): The park ID that the person stayed at.\n",
    "\n",
    "Run the cell below to load the two CSV files into two Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "a2c71b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks = pd.read_csv('data/parks.csv')\n",
    "visits = pd.read_csv('data/visits.csv', parse_dates=['entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "e09e1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "29134da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f3789",
   "metadata": {},
   "source": [
    "Now, run the cell below to define two more dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "534e619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = parks.merge(visits, on='park_id', how='inner')\n",
    "df2 = parks.merge(visits, on='park_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61494be7",
   "metadata": {},
   "source": [
    "Write Python code below to compute each of the following desired results using the dataframes `parks`, `visits`, `df1`, and `df2`. Unless otherwise stated, all number and boolean results should be numpy types (e.g. `np.float64` instead of `float`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146f6cf",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "**(3 points)**\n",
    "Find the number of times that Amina Garcia visited a national park.\n",
    "Store your result in the variable `amina` as a Python `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20c165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1bbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831efc7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41667b",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "**(4 points)**\n",
    "Find the year with the least number of park visits in this dataset.\n",
    "Store your result in the variable `least`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7d124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd29e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87398ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfa2ee",
   "metadata": {},
   "source": [
    "### Question 3 \n",
    "**(5 points)**\n",
    "Find the names of all national parks that were **never** visited by someone in this dataset.\n",
    "Store your result in the variable `never` as a `numpy` array of strings.\n",
    "(If all national parks were visited, `never` should be an empty numpy array.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78014ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890b556",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e59fcd",
   "metadata": {},
   "source": [
    "## Dog-gone data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523772bc",
   "metadata": {},
   "source": [
    "Sam was analyzing the parks data when his dog Bentley ate his data (again!).\n",
    "Sam's table is stored in a dataframe called `df` that is now missing some values in the `duration` column.\n",
    "Your task is to impute the missing duration values as effectively as possible, such that the mean and the variance of the `duration` column after imputation is as close as possible to the original data.\n",
    "First, start by running the cell below to load in Sam's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "4dfc86d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/visits_missing_a.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e8cea",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "**(1 point)**\n",
    "What proportion of values are missing in the `duration` column?\n",
    "Store your result in the variable `prop_missing` as a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cee2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125af76b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db384da",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c36a7",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "**(2 points)**\n",
    "We've imported the following functions from `dsc80_utils.py`. \n",
    "\n",
    "- `permutation_test(data, col, group_col, test_statistic, N)`\n",
    "- `diff_in_means(data, col, group_col)`\n",
    "- `tvd(data, col, group_col)`\n",
    "- `ks(data, col, group_col)`\n",
    "\n",
    "As usual, you can view the complete docstrings for these functions by adding a `?` after the function name, for example, by running:\n",
    "\n",
    "```python\n",
    "permutation_test?\n",
    "```\n",
    "\n",
    "Now, run the following cell, which runs a permutation test for the following hypotheses:\n",
    "\n",
    "- Null: The distribution of `entry_hour` is the same for park entries in 2020 and the rest of the years.\n",
    "- Alternative: The distribution of `entry_hour` is different for park entries in 2020.\n",
    "Using the results from `permutation_test`, compute the p-value and store it in `pval`. Then, set `reject` to `True` if you reject the null hypothesis or `False` if you fail to reject the null at the 0.01 significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f195c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da0ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384e5c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7189c25",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**(3 points)** Run a permutation test to determine whether the missingness of `duration` is missing at random (MAR) dependent on `entry_hour`. \n",
    "Store your p-value in the variable `hour_pval`, and your conclusion in `hour_mar` at the 0.01 significance level (`True` if `duration` is MAR dependent on `entry_hour` and `False` if not). Be sure to choose the appropriate test statistic for this hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2654e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fcbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ac755",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e7a9e",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "**(4 points)** Now, run a permutation test to determine whether the missingness of `duration` is missing at random (MAR) dependent on **any other column** in `df`: `region`, `entry_year`, and `entry_month`. Use a 0.01 significance level.\n",
    "\n",
    "For each permutation test, choose the appropriate test statistic, and store your p-values in the dictionary `pvals`. The dictionary should have one key for each column (`region`, `entry_year`, and `entry_month`). Each value in the dictionary should be the p-value for the hypothesis test to determine whether `duration` is MAR on that column.\n",
    "\n",
    "Then, mark each column that `duration` is MAR on in the dictionary `mar_cols` (`True` if `duration` is MAR on that column and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e773f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c86c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "f6e9578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't delete this cell, but do run it -- it is needed for the autograder tests\n",
    "mar_cols = mar_cols_function()\n",
    "pval = pvals_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12fd5a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd827ff8",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "**(3 points)** Impute the missing `duration` values using probabilistic imputation, conditional on the columns that `duration` is MAR on.\n",
    "You should make use of the `prob_impute` function defined below (which is copied directly from lecture).\n",
    "\n",
    "Store your result in the variable `df_imputed`, which is a copy of `df` with one extra column called `imputed` containing the imputed duration values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "1956a2e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_impute(s):\n",
    "    s = s.copy()\n",
    "    num_null = s.isna().sum()\n",
    "    fill_values = np.random.choice(s.dropna(), num_null)\n",
    "    s[s.isna()] = fill_values\n",
    "    return s\n",
    "def df_imputed_function():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46dd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53aae36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "121b7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't delete this cell, but do run it -- it is needed for the autograder tests\n",
    "df_imputed = df_imputed_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eadccdb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c24c7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd8d09",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> amina = amina_function()\n>>> type(amina) == int\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(least_function() > 2000)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> #visits['entry'].dt.year.value_counts()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(never_function(), np.ndarray)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all(isinstance(i, str) for i in never_function())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(prop_missing_function(), np.floating)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> bool(0 < prop_missing_function() < 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(p_val_function(), np.floating)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> isinstance(reject_function(), bool)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(hour_pval_function(), np.floating)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(hour_mar_function(), bool)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(pval, dict)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> set(pval.keys()) == {'region', 'entry_year', 'entry_month'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> all(isinstance(i, np.floating) for i in pval.values())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> all(0 <= i <= 1 for i in pval.values())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> isinstance(mar_cols, dict)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> set(mar_cols.keys()) == {'region', 'entry_year', 'entry_month'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> all(isinstance(i, (bool, np.bool_)) for i in mar_cols.values())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(df_imputed, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> set(df_imputed.columns) == {'region', 'duration', 'entry_year', 'entry_month', 'entry_hour', 'imputed'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> df_imputed.shape == (5000, 6)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
